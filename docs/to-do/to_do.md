---
layout: default
title: TODO List
nav_order: 6
has_children: false
---

# TODO List waiting for further construction
{: .no_toc }

## Table of contents
{: .no_toc .text-delta }

- TOC
{:toc}

---

# Useful command in linux

## pgrep
Used to find pid with given name or condition
```sh
pgrep chrome
```

## kill
Kill a pid with given siganl

```sh
# Find pid of my_program and kill with abort signal
kill -s SIGABRT ${pgrep my_program}
```

## cpack



# Core dump

## What is this


## How-TO

### Save core-dump to a file
* Set block size for core dump file
```sh
# 1. Get your chunk size
# where %s means only show chunk size.
# stat will show a lot of file/dir info
# -f means using format, which allows special character like %s
stat -fc %s .

# 2. Calculate max chunks you want for all core dump files
# chunk number for 1 MB = 1*1024*1024 / chunk_size

# 3. Set max size
sudo vim /etc/security/limits.conf
# Add 2 lines as hard limit + soft limit for specified user/group
# usr_name hard core chunk_number
# usr_name soft core chunk_number
# e.g.
musterman hard Core 1280
musterman soft Core 1280

# 4. Reboot
reboot

# 5. Check size
ulimit -c
```

Or, a easy way is to set limit to unlimit:
```sh
ulimit -c unlimited
```


* Set core dump file name pattern
```sh
# sudo sysctl -w kernel.core_pattern="your pattern"
# e.g.
sudo sysctl -w kernel.core_pattern="/tmp/core_dump.%e.%p"
# %e is program name
# %p is pid

# If give file name as pattern, then file will be saved with the name under current dir

# Now the core dump file will be saved with name to given position.
```
Alternatively, you could also change in `core_pattern` file directly
```sh
# This will write core.%e.%p to /proc/sys/kernel/core_pattern
# This means everytime generate files with this pattern
# The default pattern inside is usually start with |, 
# means run the command after | with fixed pattern.
sudo bash -c 'echo core.%e.%p > /proc/sys/kernel/core_pattern' 
```



### Handle core-dump
* Debug using gdb with know core dump file
```sh
# This will print out in which part core dump was occured
gdb <executable_file> <core_dumpe_file> 
```
* Debug with core-dump

## Reference
* [managing core dumps](https://www.baeldung.com/linux/managing-core-dumps)
* [debugging core dump segementation fault gpd](https://www.bogotobogo.com/cplusplus/debugging_core_memory_dump_segmentation_fault_gdb.php)


# SSH connection
## What is this
A safter way to connect to other server. SSH could provide terminal connection, remote-desk connection, reverse/tunneling, etc.

## HOW-TO
### Basic

### Use SSH as proxy in browser

## Notice
### Safety


## Reference
* [why are some ports risky and how do you secure them](https://www.howtogeek.com/devops/why-are-some-ports-risky-and-how-do-you-secure-them/)



# Cron in Linux
## What is this
In linux, cron will start defined jobs at given time periods. Jobs could be defined by each user themselves. 

To see the service status:
```sh
sudo systemctl status cron.service
```

Cron save the jobs to be executed in several path, some of them are:
`cracklib/`,
`cron.daily`,
`cron.monthly`,
`cron.weekly`,
`cron.d`,
`cron.hourly`,
`crontab`

## How-To
* List cron jobs for current user:

```sh
crontab -l
```

* Add jobs for current user 
```sh
crontab -e
```
* For each command, a time will be written in the front.

```sh
# 0-59 min, 0-23 hour, 1-31 day, 1-12 month, 0-6 week-day. * for all time
* * * * * command-to-be-executed 
```

## Notice
For some linux version, cron were used for privilege escalation attack. 

Attacker will navigate to `/etc/cron.d` folder and use the core dump file generated by the attack program in this folder as an attack job to execute command with sudo privilege.

## Reference

* [cron jobs and crontab on linux explained](https://devconnected.com/cron-jobs-and-crontab-on-linux-explained/)
* [privilege escalation attack use cron](https://en.wikipedia.org/wiki/Privilege_escalation)


# English Usage:
It is worth noting: 值得注意的是
Sth1 shed some light on sth2:  sth1 揭示了 sth2
substantially: 可观的，大量的
attaining: 获得
asymptotic performance: 渐进特性（即收敛后的结果）
under-appreciated detail: 被忽视的， 未被重视的 细节
monotonically: 单调的
preliminary: 初步的
unprecedented: 史无前例的
discrepancy: 差异，不同
to this end: 为此，因此
To the best of our knowledge: 尽我们所能
, where sth denotes sth: （用于表达次公式是用表达某个变量的...） 
which is referred to as: 被称作
expound/elaborate/discribe: 陈述，描述
be disentangled: 被分解为
superscript: 上标
omitted: 被省略
conforms xx distribution: 符合...分布
fidelity: 保真度


# Large Language Model:

## Model
### vicuna 13B
Fined tuned based on LLaMA with shared ChatGPT data on ShareGPT.

**Self-Instruct**: A project that use existed language model and 175 seed instruction tasks to automatically generate more instruction-based conversation data. Use these data to improve language model's instruction following ability.

**Stanford Alpaca**: A project that fined tuned based on LLaMA with 52k instruction-following data. But it rewrite some prompt.

Trained using **SkyPilot**: Build training process with unified interface, available on multi-platform.

**FastChat** github 可安装 不同版本。 

### LLaMA

### gpt4-x-alpaca


## Concepts:
### 字节对编码 (BPE) 算法:
Used in Tokenizer by LLaMA.

### SentencePiece:

### Prompt:
Use pre-defined natural language template to guide LM creating desired output.

E.g. one prompt could be : `{input} is so {label}.`. During fine tuning, the input could be a paragraph of comment, while the label could be the description of this comment, e.g. good, bad.  

(Link)[https://zhuanlan.zhihu.com/p/438766871]


